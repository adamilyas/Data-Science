{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffa2cf84139a77e35b6670125472be2a",
     "grade": false,
     "grade_id": "cell-50a11b583482297d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Machine Learning: Basic Principles 2018\n",
    "\n",
    "# Feature Learning\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "In this exercise you will learn how to use __principal component analysis (PCA)__ for __feature learning__. PCA allows us to optimally compress the information contained in high-dimensional raw feature vectors into a small set of new features, which are called __principal components__ (PC). These new features are then used for formulating and solving an overall ML problem  (regression or classification). Using the PCs as features (instead of the raw feature vectors) is beneficial in terms of computational complexity (using smaller feature vectors means less computation) and statistical properties of the learning method (smaller feature vectors reduces the risk of overfitting). Moreover, using the first two PCs allows to visualize high-dimensional data points conveniently using a scatter plot.\n",
    "\n",
    "\n",
    "### Exercise Contents\n",
    "\n",
    "1. [Data](#data)\n",
    "2. Exercise\n",
    "    - 2.1 [Implement Principal Component Analysis (PCA)](#Q1)\n",
    "    - 2.2 [Compression vs.  Information Loss](#Q2)\n",
    "    - 2.3 [PCA for Data Visualization](#Q3)\n",
    "\n",
    "### Keywords\n",
    "`principal component analysis (PCA)`, `eigenvalue decomposition`, `data visualization`, `overfitting`.\n",
    "\n",
    "## 1. The Data\n",
    "<a id=\"data\"></a>\n",
    "In this exercise we will work with a dataset containing fruits, more precisely apples and bananas. \n",
    "The available dataset consists of $N=30$ pictures, saved in the folder named `fruits`:\n",
    "* 15 images of apples (`image_1.jpg` to `image_15.jpg`)\n",
    "* 15 images of bananas (`image_16.jpg` to `image_30.jpg`)\n",
    "\n",
    "The images are color images, but for this image we convert them to grayscale. This means that each pixel has one value between 0 and 255, where 0 means white and 255 means black. The size of the images is $50x50$ pixels. We convert each image to an array of length 2500. \n",
    "\n",
    "The following code block loads the images and stores them in the array `datamatrix`. We also plot six of the images, to get a feeling for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "812f251af527cd5c9f5457f6a81dcd65",
     "grade": false,
     "grade_id": "cell-3c74bce761b38a5b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "samplesize = 30\n",
    "dataset = np.zeros((samplesize,50,50),dtype=np.uint8)\n",
    "\n",
    "for i in range(1,samplesize+1):\n",
    "    # with convert('L') we convert the images to grayscale\n",
    "    img = Image.open('./fruits/%s.jpg'%(str(i))).convert('L')\n",
    "    dataset[i-1] = np.array(img,dtype=np.uint8)\n",
    "  \n",
    "# Reshaping the images as vectors of length 2500 and store them in the rows of \"datamatrix\"\n",
    "datamatrix = np.reshape(dataset,(samplesize,-1))\n",
    "print(\"The shape of the datamatrix is\", datamatrix.shape)\n",
    "\n",
    "fig,ax = plt.subplots(3,2,figsize=(10,10),gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        ax[i,j].imshow(dataset[12+i+j*3], cmap='gray')\n",
    "        ax[i,j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3faccc250ea795a66f0294dd65c4f815",
     "grade": false,
     "grade_id": "cell-1ce2ebc346939f4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 2 Exercise\n",
    "The actual exercise starts from here and it has been divided into 3 tasks:\n",
    "* 2.1 Implement Principal Component Analysis (PCA)\n",
    "* 2.2 Compression vs.  Information Loss\n",
    "* 2.3 PCA for Data Visualization\n",
    "\n",
    "\n",
    "## 2.1 Implement Principal Component Analysis (PCA)\n",
    "<a id=\"Q1\"></a>\n",
    "\n",
    "Each fruit image is represented by the raw data vector  $\\mathbf{z}^{(i)} \\in \\mathbb{R}^{D}$, which contains the grayscale values for each of the $D=2500$ pixels. \n",
    "The aim of PCA is to find the (in a certain sense) optimal compression matrix $\\mathbf{W} \\in \\mathbb{R}^{d \\times D}$, with the number of features $d \\ll D$, that transforms the raw data vector $\\mathbf{z}^{(i)}$ to a short feature vector $\\mathbf{x}^{(i)}$. \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}^{(i)}=\\mathbf{W} \\mathbf{z}^{(i)}. \n",
    "    \\tag{1} \n",
    "    \\label{eq:1}\n",
    "\\end{equation}\n",
    "\n",
    "The optimal compression matrix $\\mathbf{W}$ is the matrix which gives the lowest loss of information. We quantify the loss of information when replacing the original raw data vectors $\\mathbf{z}^{(i)}$ with the new feature vectors $\\mathbf{x}^{(i)}$ by the smallest possible  error achieved by reconstructing (in a linear fashion) the raw data vector $\\mathbf{z}^{(i)}$ from the new feature vector $\\mathbf{x}^{(i)}$. The reconstruction error is given by: \n",
    "\n",
    "\\begin{align}\n",
    " \\mathcal{E}(\\mathbf{W}) & = \\min_{\\mathbf{R} \\in \\mathbb{R}^{D \\times d}} \\dfrac{1}{N} \\sum_{i=1}^{N} \\| \\mathbf{z}^{(i)} - \\mathbf{R} \\mathbf{x}^{(i)} \\|^{2}  \\nonumber \\\\ \n",
    " & \\stackrel{\\eqref{eq:1}}{=} \\min_{\\mathbf{R} \\in \\mathbb{R}^{D \\times d}} \\dfrac{1}{N} \\sum_{i=1}^{N} \\| \\mathbf{z}^{(i)} - \\mathbf{R} \\mathbf{W} \\mathbf{z}^{(i)} \\|^{2}.\n",
    " \\label{eq:2}\n",
    " \\tag{2}\n",
    "\\end{align} \n",
    "\n",
    "Note that the optimal reconstruction matrix $\\mathbf{R}$ depends on the given compression matrix $\\mathbf{W}$. However, we are not interested in the optimal reconstruction matrix itself but only in the associated minimum reconstruction error $\\mathcal{E}(\\mathbf{W})$. In order to construct the optimal compression matrix $\\mathbf{W}$ which minimizes the reconstruction error $\\mathcal{E}(\\mathbf{W})$, we will use the eigenvalue decomposition of the sample covariance matrix $\\mathbf{Q}$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Q} = \\dfrac{1}{N} \\mathbf{Z}^{T} \\mathbf{Z} \\mbox{ with data matrix } \\mathbf{Z}=\\big(\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(N)} \\big)^{T} \\in \\mathbb{R}^{N \\times D}. \n",
    "\\label{eq:3}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Since the matrix $\\mathbf{Q}$ is **positive semi-definite**, it has an eigenvalue decomposition of the form \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Q} = \\big( \\mathbf{u}^{(1)},\\ldots,\\mathbf{u}^{(D)}\\big) {\\rm diag} \\{\\lambda^{(1)},\\ldots,\\lambda^{(D)} \\} \n",
    " \\big( \\mathbf{u}^{(1)},\\ldots,\\mathbf{u}^{(D)}\\big)^{T}.\n",
    "\\end{equation} \n",
    "\n",
    "Here, the orthonormal vectors $\\mathbf{u}^{(r)}$ are eigenvectors of $\\mathbf{Q}$, which correspond to the decreasingly ordered eigenvalues $\\lambda^{(1)} \\geq \\lambda^{(2)} \\geq \\ldots \\geq \\lambda^{(D)}$ of $\\mathbf{Q}$. \n",
    "\n",
    "It can be shown that the reconstruction error $\\mathcal{E}(\\mathbf{W}_{\\rm PCA}) = \\min_{\\mathbf{W} \\in \\mathbb{R}^{d \\times D}} \\mathcal{E}(\\mathbf{W})$ (see \\eqref{eq:2}) can only be minimal for the particular compression matrix \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:4}\n",
    "\\tag{4}\n",
    "\\mathbf{W}_{\\rm PCA} = \\big( \\mathbf{u}^{(1)},\\ldots,\\mathbf{u}^{(d)} \\big)^{T},\\in \\mathbb{R}^{d \\times D}\n",
    "\\end{equation}\n",
    "\n",
    "whose rows are the eigenvectors $\\mathbf{u}^{(1)},\\ldots,\\mathbf{u}^{(d)}$ of $\\mathbf{Q}$ corresponding to the \n",
    "$d$ largest eigenvalues $\\lambda^{(1)},\\ldots,\\lambda^{(d)}$ of $\\mathbf{Q}$. \n",
    "\n",
    "Moreover, it can be shown that the minimum reconstruction error $\\mathcal{E}(\\mathbf{W}_{\\rm PCA})$ is achieved \n",
    "for the reconstruction matrix $\\mathbf{R} = \\mathbf{W}_{\\rm PCA}^{T}$. This simplifies the reconstruction error $\\mathcal{E}(\\mathbf{W}_{\\rm PCA})$ to \n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:5}\n",
    "\\tag{5}\n",
    "\\mathcal{E}(\\mathbf{W}_{\\rm PCA}) = \\sum_{r = d+1}^{D} \\lambda^{(r)}. \n",
    "\\end{equation} \n",
    "\n",
    "For an intuitive explanation of PCA, this is an excellent StackExchange answer: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\n",
    "\n",
    "\n",
    "#### Tasks\n",
    "- Implement a Python function `compute_pca`, which has two input arguments: the data matrix $\\mathbf{Z}$ and the number $d$ of new features (principal components). In particular:\n",
    " \n",
    "    - compute the sample covariance matrix $\\mathbf{Q}=\\dfrac{1}{N} \\mathbf{Z}^{T} \\mathbf{Z}$ (see \\eqref{eq:3})\n",
    "    - compute eigenvectors $(\\mathbf{u}^{(1)},...,\\mathbf{u}^{(D)})$ and corresponding eigenvalues $(\\lambda^1,...,\\lambda^D)$ of $\\mathbf{Q}$. (see the Round 1 - Introduction notebook for a review of eigendecomposition in Python). \n",
    "    - sort the eigenvalues in a decreasing order: $\\lambda^{(1)} \\geq \\lambda^{(2)} \\geq...\\lambda^{(D)} \\geq 0$.\n",
    "    - choose $d$ eigenvectors corresponding to the $d$  largest eigenvalues to form the compression matrix $\\mathbf{W}_{\\rm PCA}=(\\mathbf{u}^{(1)},...,\\mathbf{u}^{(d)})^{T} \\in \\mathbb{R}^{d \\times D}$ \n",
    "    - return the compresssion matrix $\\mathbf{W}_{\\rm PCA}$ (see \\eqref{eq:4}) and all eigenvalues of $\\mathbf{Q}$.\n",
    "\n",
    "**Note:** You may get eigenvectors and eigen values in complex data type (a+bi format), this occurs due to small numerical errors in calculation. The imaginary term is usually zero and can be ignored. The real part can be extracted using `.real`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71dc412a69f8cfb9a39947968346b043",
     "grade": false,
     "grade_id": "cell-5a7e020e5c3b71c2",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_pca(Z, d):\n",
    "    # Input: the N by D data matrix Z, the number of components d\n",
    "    # Output: a d by D matrix W_pca, and all eigenvalues of Q\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    \n",
    "    # step1: compute the sample cov. matrix Q\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    #step2: compute the eigenvalues and eigenvectors (see introduction notebook)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #step3: Sort the eigenvectors by decreasing eigenvalues, choose the d largest eigenvalues, form W_pca\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return W_pca.real,eigvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e07422e726087814fe134f00f7bb682e",
     "grade": false,
     "grade_id": "cell-e84eac83a215b2b4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 2.2. Compression vs Information Loss\n",
    "<a id=\"Q2\"></a>\n",
    "We are going to use the PCA function you have implemented above to compute the new feature vectors $\\mathbf{x}^{(i)}$ for the raw feature vectors $\\mathbf{z}^{(i)}$ of the images ([see Section 1 “The Data”](#data)). We will try out using different values of $d$, the length of the new feature vectors, in order to see that there is a trade-off between compression (which requires small values of $d$) and having small loss of information and reconstruction error (which requires large values of $d$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fd0a7e5d66c8784be1b6a09b06e5d0e",
     "grade": true,
     "grade_id": "cell-34aaf9df0f5c40f3",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the first 50 components of PCA and its eigenvectors\n",
    "# We divide the datamatrix by 255 since PCA works better on normalized data\n",
    "PCA,eigvalues = compute_pca(datamatrix/255.,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee155ebe92dc832486f1929303e2463c",
     "grade": false,
     "grade_id": "cell-c5863ce018ffc1bc",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To get an insight of how the reconstruction error changes when $d$ increases, we make a plot of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3737856b03740e59cf49f907245abc8",
     "grade": false,
     "grade_id": "cell-5509388d0cb90aac",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_error(eigvalues,max_d):\n",
    "    x=range(1,max_d+1)\n",
    "    errors=[sum(eigvalues[d:]) for d in x]\n",
    "    plt.plot(x,errors)\n",
    "    plt.xlabel('Number of principal components $d$')\n",
    "    plt.ylabel('Reconstruction error $\\mathcal{E}$')\n",
    "    plt.title('Number of principal components vs the reconstruction error')\n",
    "    plt.show()\n",
    "    \n",
    "# plot the number of principal components vs the reconstruction error\n",
    "plot_error(eigvalues,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99bd38f0c2de30f7a11b6224e2ba3720",
     "grade": false,
     "grade_id": "cell-83a76b27090a2bb0",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now let's try to understand what PCA is doing. For this we plot a few of the rows $\\mathbf{u}^{(r)}$ of the optimal compression matrix $\\mathbf{W}_{\\rm PCA}$. These rows, which are eigenvectors of the sample covariance matrix $\\mathbf{Q}$, are sometimes called \"principal directions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abe38469aab0f430c2b3eb8c14fe298b",
     "grade": false,
     "grade_id": "cell-c419ed920aa5c0c7",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_princ_comp(PCA):\n",
    "    fig,ax = plt.subplots(1,5,figsize=(15,15))\n",
    "    # select the principal components we are plotting\n",
    "    # You can change these to see what other components look like\n",
    "    plot_pd = [0,4,9,14,19]\n",
    "\n",
    "    for i in range(len(plot_pd)):\n",
    "        ax[i].imshow(np.reshape(PCA[plot_pd[i]]*255,(50,50)),cmap='gray')\n",
    "        ax[i].set_title(\"Principal Direction %d\"%(plot_pd[i]+1))\n",
    "    plt.show()\n",
    "\n",
    "plot_princ_comp(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a50a6ae54b8a9da9f77b6a9db054cfa4",
     "grade": false,
     "grade_id": "cell-45b5e1214d2b641c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To get an even better understanding, we want to create the reconstructed images using $ d $ components, i.e. $\\mathbf{R} \\mathbf{x}^{(i)}$. \n",
    "\n",
    "We plot the same images as at the start of the notebook, but now for $d=1,5,50$. \n",
    "\n",
    "Can you see the differences? You can try yourself how a different number of $d$ changes the output! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "590ecfbebb9ec195093ff7f3c65a3a05",
     "grade": false,
     "grade_id": "cell-b88fd65a90406180",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.gridspec as gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### Input:\n",
    "##  X: Dataset\n",
    "##  d: number of dimensions\n",
    "##  n_pics: number of pics per class (Apple, Banana). Min 1, Max 15\n",
    "def plot_reconstruct(X,d, n_pics=5):\n",
    "    # x=w*z\n",
    "    X_pca = np.matmul(PCA[:d,:],X[:,:,None])\n",
    "    # x_reversed=r*x\n",
    "    X_reversed = np.matmul(PCA[:d,:].T, X_pca)[:,:,0]\n",
    "    \n",
    "    # Setup Figure size that scales with number of images\n",
    "    fig = plt.figure(figsize = (4,max(8*np.log(n_pics),5)))\n",
    "    \n",
    "    # Setup a (n_pics,2) grid of plots (images)\n",
    "    gs = gridspec.GridSpec(n_pics, 2)\n",
    "    gs.update(wspace=0.0, hspace=0.0)\n",
    "    for i in range(n_pics):\n",
    "        for j in range(0,2):\n",
    "            # Add a new subplot\n",
    "            ax = plt.subplot(gs[i,j])\n",
    "            \n",
    "            # Insert image data into the subplot\n",
    "            ax.imshow(np.reshape(X_reversed[i+(15*j)],(50,50)),cmap='gray',interpolation='nearest')\n",
    "            \n",
    "            # Remove x- and y-axis from each plot\n",
    "            ax.set_axis_off()\n",
    "    \n",
    "    # Setup a visual divider between Apples and Bananas (i.e. a black line between plots)\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    column_divider = Line2D((0.5,0.5),(0.13,0.91),color='black', transform=fig.transFigure)\n",
    "    fig.lines.extend([column_divider])\n",
    "    \n",
    "    # Setup column title for Apples and Bananas.\n",
    "    plt.subplot(gs[0,0]).set_title('Apples', size='large', y=1.08)\n",
    "    plt.subplot(gs[0,1]).set_title('Bananas', size='large', y=1.08)\n",
    "    \n",
    "    # Render the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values of d to plot for. You can change these to experiment\n",
    "# If you want to print different amount of pictures, you can change the value of n_pics. (1-15)\n",
    "num_com=[1,5,50]\n",
    "for d in num_com:\n",
    "    print(\"Reconstructed image using %d principal components:\"%(d))\n",
    "    plot_reconstruct(datamatrix,d,n_pics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ab7775b14df3e8c200d67d5e4be3251",
     "grade": false,
     "grade_id": "cell-44992b7e2e2296cb",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 2.3 PCA for Data Visualization\n",
    "<a id=\"Q3\"></a>\n",
    "\n",
    "One of the uses of PCA is visualization. We do PCA with $d=2$  in order to visualize the dataset $\\mathbf{z}^{(1)},\\ldots,\\mathbf{z}^{(N)}$. Indeed, the resulting feature vectors $\\mathbf{x}^{(1)} = W_{\\rm PCA} \\mathbf{z}^{(1)}, \\ldots, \\mathbf{x}^{(N)} = W_{\\rm PCA} \\mathbf{z}^{(N)} \\in \\mathbb{R}^{2}$\n",
    "can be visualized as a scatter plot whose x-axis represents the first principal component $x_{1}^{(i)}$ and y-asix represents the second principal component $x_{2}^{(i)}$.\n",
    "\n",
    "Recall that in Round 3 - Classification, we displayed similar scatter plots based on the manually chosen features (the colors). We now used PCA to find the features instead of manually choosing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9880659c9d4abf4398af25ebb9938bb2",
     "grade": false,
     "grade_id": "cell-6e5e08f4001787a1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_scatter(PCA,Z):\n",
    "    # get x for d=2\n",
    "    X_2d = np.matmul(PCA[:2,:],Z[:,:,None])[:,:,0]\n",
    "\n",
    "    plt.scatter(X_2d[:15,0],X_2d[:15,1],c='r',marker='o',label='Apple')\n",
    "    plt.scatter(X_2d[15:,0],X_2d[15:,1],c='y',marker='^',label='Banana')\n",
    "    plt.legend()\n",
    "    plt.xlabel('First principal component')\n",
    "    plt.ylabel('Second principal component')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_scatter(PCA,datamatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
